{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow:\n",
    "\n",
    "1. We first do exact schema clustering\n",
    "2. Apply PPO within each cluster\n",
    "3. Merge clusters one at a time to connect\n",
    "    1. Use PPO First\n",
    "    2. If schema match during merge, try join check\n",
    "    3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lineage/similarity.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from lineage import graphs, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_DIR = '/media/suhail/Data/experiments/reexec/res/'\n",
    "#BASE_DIR = '/home/suhail/Projects/sample_workflows/million_notebooks/selected/'\n",
    "BASE_DIR = '/home/suhail/ok/'\n",
    "#BASE_DIR = '/home/suhail/Projects/relic/primitives/python/generator/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB_NAME = 'nb_331056.ipynb'\n",
    "NB_NAME = 'nb_289248.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "# Jaccard Distance First\n",
    "\n",
    "#Duplicate function\n",
    "def set_jaccard_distance(set1,set2):\n",
    "    intersect = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return 1-(len(intersect)/len(union))\n",
    "\n",
    "def select_merge_candidates(schemas):\n",
    "    distance_dict = defaultdict(list)\n",
    "    for combo in itertools.combinations(schemas,2):\n",
    "        distance_dict[set_jaccard_distance(*combo)].append(combo)\n",
    "    \n",
    "    return distance_dict[min(distance_dict)], min(distance_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(schema_dict, schema1, schema2):\n",
    "    \n",
    "    union = schema1.union(schema2)\n",
    "    elements = schema_dict[schema1] + schema_dict[schema2] \n",
    "    \n",
    "    #print('BEFORE')\n",
    "    #print(schema_dict)\n",
    "    #print(union)\n",
    "    #print(schema_dict[union])\n",
    "    \n",
    "    try:\n",
    "        del(schema_dict[schema1])\n",
    "        del(schema_dict[schema2])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    schema_dict[union] = elements\n",
    "    #print('AFTER')\n",
    "    #print(schema_dict)\n",
    "    \n",
    "    return schema_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as ds\n",
    "import clustering\n",
    "\n",
    "\n",
    "df_dict = ds.build_df_dict_dir(BASE_DIR+NB_NAME+'/artifacts/')\n",
    "df_dict\n",
    "schema_dict = clustering.exact_schema_cluster(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates,score = select_merge_candidates(schema_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level\n",
      "level\n",
      "level\n"
     ]
    }
   ],
   "source": [
    "while(len(schema_dict) > 1):\n",
    "    print('level')\n",
    "    schemas, score = select_merge_candidates(schema_dict.keys())\n",
    "    schema_dict = merge_clusters(schema_dict, schemas[0][0], schemas[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {frozenset({'amount',\n",
       "                        'calaccess_committee_id',\n",
       "                        'calaccess_filing_id',\n",
       "                        'calaccess_prop_id',\n",
       "                        'ccdc_committee_id',\n",
       "                        'ccdc_prop_id',\n",
       "                        'committee_name',\n",
       "                        'committee_name_x',\n",
       "                        'committee_name_y',\n",
       "                        'committee_position',\n",
       "                        'contributor_city',\n",
       "                        'contributor_employer',\n",
       "                        'contributor_firstname',\n",
       "                        'contributor_fullname',\n",
       "                        'contributor_is_self_employed',\n",
       "                        'contributor_lastname',\n",
       "                        'contributor_occupation',\n",
       "                        'contributor_state',\n",
       "                        'contributor_zip',\n",
       "                        'date_received',\n",
       "                        'ocd_prop_id',\n",
       "                        'prop_name'}): ['top_supporters.csv',\n",
       "              'props.csv',\n",
       "              'prop.csv',\n",
       "              'oppose.csv',\n",
       "              'noncacontribs.csv',\n",
       "              'support.csv',\n",
       "              'merged.csv',\n",
       "              'contribs.csv']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lineage import similarity, graphs\n",
    "\n",
    "def intra_cluster_similarity(df_dict, clusters, threshold=0.0001):\n",
    "    pairwise_jaccard = []\n",
    "    for cluster in clusters.values():\n",
    "        batch = {k: df_dict[k] for k in cluster}\n",
    "        pw_batch = similarity.get_pairwise_similarity(batch, similarity.compute_jaccard_DF, threshold=threshold)\n",
    "        pairwise_jaccard.extend(pw_batch)\n",
    "    return pairwise_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import nppo\n",
    "\n",
    "\n",
    "def lineage_inference_agglomerative(nb_name=NB_NAME, base_dir=BASE_DIR,\n",
    "                      pre_cluster=False,\n",
    "                      index=True, threshold=0.0001,\n",
    "                      join_edges=False,\n",
    "                      group_edges=False,\n",
    "                      ):\n",
    "\n",
    "    wf_dir = base_dir+nb_name\n",
    "\n",
    "    if index:\n",
    "        artifact_dir = wf_dir+'/artifacts/'\n",
    "    else:\n",
    "        artifact_dir = wf_dir+'/artifacts_1/'\n",
    "\n",
    "    #Output Directory\n",
    "    result_dir = wf_dir+'/inferred/'\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # Output Files\n",
    "    schema_file = result_dir+'schema_matching.csv'\n",
    "    row_file = result_dir+'row_matching.csv'\n",
    "    cluster_file = result_dir+'clusters.csv'\n",
    "    \n",
    "    \n",
    "    # Prepare Dataframe for results\n",
    "    pr_df = pd.DataFrame(columns = ['nb_name', 'index', 'numclusters',  \n",
    "                                    'distance_metric', 'edges_correct', \n",
    "                                    'edges_missing', 'edges_to_remove',\n",
    "                                    'join_edges', 'precision', 'recall', 'F1',\n",
    "                                    'missing_files'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Load Dataset\n",
    "    dataset = ds.build_df_dict_dir(artifact_dir)\n",
    "    \n",
    "    \n",
    "    # Load Ground Truth:\n",
    "    g_truth = nx.read_gpickle(wf_dir+'/'+nb_name+'_gt_fixed.pkl')\n",
    "    \n",
    "    # Check for files in the ground truth that are missing in file list\n",
    "    missing_files = ds.check_csv_graph(artifact_dir, g_truth)\n",
    "\n",
    "    # Cluster for visualization\n",
    "    clusters = clustering.exact_schema_cluster(dataset)\n",
    "    clustering.write_clusters_to_file(clusters, result_dir+'clusters_with_filename.csv')\n",
    "\n",
    "    # Start with intra-cluster edges:\n",
    "    pairwise_jaccard = intra_cluster_similarity(df_dict, clusters)\n",
    "\n",
    "    pw_jaccard_graph = graphs.generate_pairwise_graph(pairwise_jaccard)\n",
    "\n",
    "    \n",
    "       \n",
    "    # Write out the Pairwise Distances as Adj list\n",
    "    nx.to_pandas_adjacency(pw_jaccard_graph,weight='weight').to_csv(\n",
    "                                                result_dir+'cell_sim.csv')\n",
    "\n",
    "    g_inferred = graphs.generate_spanning_tree(pw_jaccard_graph)\n",
    "    nx.write_edgelist(g_inferred,result_dir+'infered_mst_cell.csv',data=True)\n",
    "    \n",
    "    #Draw first graph and get results\n",
    "    cluster_dict = clustering.get_graph_clusters(result_dir+'clusters_with_filename.csv')\n",
    "    img_frames.append(graphs.generate_and_draw_graph(base_dir, nb_name, 'cell', cluster_dict=cluster_dict, join_list=None))\n",
    "    \n",
    "\n",
    "    result = graphs.get_precision_recall(g_truth,g_inferred)\n",
    "        \n",
    "    pr_df = pr_df.append({\n",
    "        'nb_name': nb_name,\n",
    "        'index': index,\n",
    "        'numclusters': len(clusters),\n",
    "        'distance_metric': 'pandas_cell',\n",
    "        'edges_correct': len(result['correct_edges']),\n",
    "        'edges_missing': len(result['to_add']),\n",
    "        'edges_to_remove': len(result['to_remove']),\n",
    "        #'join_edges': len(inferred_j_edges),\n",
    "        'precision': result['Precision'],\n",
    "        'recall': result['Recall'],\n",
    "        'F1': result['F1'],\n",
    "        'missing_files': len(missing_files)\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    # Write out inferred graph\n",
    "    nx.write_edgelist(g_inferred,result_dir+'infered_mst_cell.csv',data=True)\n",
    "\n",
    "    #Clustering Loop Starts here\n",
    "    while(len(clusters) > 1):\n",
    "        print('Num Clusters:', len(clusters))\n",
    "        candidates,score = select_merge_candidates(clusters.keys())\n",
    "\n",
    "        clusterset1, clusterset2 = clusters[candidates[0][0]], clusters[candidates[0][1]]\n",
    "\n",
    "        src, dst, score = similarity.get_pairs_similarity(df_dict, clusterset1, clusterset2)[0]\n",
    "        print('Adding Edge:', src, dst, score)\n",
    "        \n",
    "        if(score > 0):\n",
    "            g_inferred.add_edge(src, dst, weight=score)\n",
    "\n",
    "        nx.write_edgelist(g_inferred,result_dir+'infered_mst_cell.csv',data=True)\n",
    "        clusters = merge_clusters(clusters, candidates[0][0], candidates[0][1])\n",
    "        \n",
    "        # Draw inferred graph image:\n",
    "        if(len(clusters) > 1):\n",
    "            clustering.write_clusters_to_file(clusters, result_dir+'clusters_with_filename.csv')\n",
    "            cluster_dict = clustering.get_graph_clusters(result_dir+'clusters_with_filename.csv')\n",
    "        else:\n",
    "            cluster_dict = None\n",
    "        img_frames.append(graphs.generate_and_draw_graph(base_dir, nb_name, 'cell', cluster_dict=cluster_dict, join_list=None))\n",
    "    \n",
    "    \n",
    "        #Compute PR after merge\n",
    "        \n",
    "        result = graphs.get_precision_recall(g_truth,g_inferred)\n",
    "        \n",
    "        pr_df = pr_df.append({\n",
    "            'nb_name': nb_name,\n",
    "            'index': index,\n",
    "            'numclusters': len(clusters),\n",
    "            'distance_metric': 'pandas_cell',\n",
    "            'edges_correct': len(result['correct_edges']),\n",
    "            'edges_missing': len(result['to_add']),\n",
    "            'edges_to_remove': len(result['to_remove']),\n",
    "            #'join_edges': len(inferred_j_edges),\n",
    "            'precision': result['Precision'],\n",
    "            'recall': result['Recall'],\n",
    "            'F1': result['F1'],\n",
    "            'missing_files': len(missing_files)\n",
    "        }, ignore_index=True)\n",
    "            \n",
    "\n",
    "    # Test for NPPOs:\n",
    "    \n",
    "    inferred_j_edges = []\n",
    "    join_list = None\n",
    "    cluster_dict = None\n",
    "\n",
    "    if join_edges:\n",
    "        print('Writing Cluster File')\n",
    "\n",
    "        print(\"Adding Join Edges\")\n",
    "        join_list = nppo.find_all_joins_df_dict(dataset)\n",
    "        print(len(join_list), \"Joins Detected\")\n",
    "        g_inferred = nppo.add_join_edges(join_list, g_inferred)\n",
    "\n",
    "\n",
    "        for join in join_list:\n",
    "            inferred_j_edges.append((join[0], join[2]))\n",
    "            inferred_j_edges.append((join[1], join[2]))\n",
    "\n",
    "\n",
    "        nppo.write_join_candidates(join_list, result_dir+'join_candidates.csv')\n",
    "\n",
    "        g_truth_j_edges = [(u,v) for u,v,d in g_truth.edges(data=True) \\\n",
    "                           if g_truth[u][v]['operation'] == 'merge' ]\n",
    "\n",
    "        #Check Join Precision/Recall\n",
    "        #print(get_join_precision_recall(g_truth_j_edges, inferred_j_edges))\n",
    "        \n",
    "        result = graphs.get_precision_recall(g_truth,g_inferred)\n",
    "\n",
    "        cluster_dict = clustering.get_graph_clusters(result_dir+'clusters_with_filename.csv')\n",
    "        img_frames.append(graphs.generate_and_draw_graph(base_dir, nb_name, 'cell', cluster_dict=cluster_dict, join_list=join_list))\n",
    "\n",
    "        pr_df = pr_df.append({\n",
    "            'nb_name': nb_name,\n",
    "            'index': index,\n",
    "            'numclusters': len(clusters),\n",
    "            'distance_metric': 'pandas_cell',\n",
    "            'edges_correct': len(result['correct_edges']),\n",
    "            'edges_missing': len(result['to_add']),\n",
    "            'edges_to_remove': len(result['to_remove']),\n",
    "            'join_edges': len(inferred_j_edges),\n",
    "            'precision': result['Precision'],\n",
    "            'recall': result['Recall'],\n",
    "            'F1': result['F1'],\n",
    "            'missing_files': len(missing_files)\n",
    "        }, ignore_index=True)\n",
    "  \n",
    "    return pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='graph pairs', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='graph pairs', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='graph pairs', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='graph pairs', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Clusters: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='cluster pairs', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Edge: merged.csv contribs.csv 0.0015397492649661011\n",
      "Num Clusters: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='cluster pairs', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Edge: props.csv support.csv 0.0073740782402199724\n",
      "Num Clusters: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='cluster pairs', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Adding Edge: top_supporters.csv props.csv 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_name</th>\n",
       "      <th>index</th>\n",
       "      <th>numclusters</th>\n",
       "      <th>distance_metric</th>\n",
       "      <th>edges_correct</th>\n",
       "      <th>edges_missing</th>\n",
       "      <th>edges_to_remove</th>\n",
       "      <th>join_edges</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>missing_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nb_name index numclusters distance_metric edges_correct  \\\n",
       "0  nb_495072.ipynb  True           4     pandas_cell             2   \n",
       "1  nb_495072.ipynb  True           3     pandas_cell             3   \n",
       "2  nb_495072.ipynb  True           2     pandas_cell             3   \n",
       "3  nb_495072.ipynb  True           1     pandas_cell             3   \n",
       "\n",
       "  edges_missing edges_to_remove  join_edges  precision  recall        F1  \\\n",
       "0             5               2         NaN   0.285714     0.5  0.363636   \n",
       "1             4               2         NaN   0.428571     0.6  0.500000   \n",
       "2             4               3         NaN   0.428571     0.5  0.461538   \n",
       "3             4               3         NaN   0.428571     0.5  0.461538   \n",
       "\n",
       "  missing_files  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x3600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = lineage_inference_agglomerative(nb_name=NB_NAME)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_frames = [Image.open(frame) for frame in img_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_frames[0].save('mexican.gif', \n",
    "                     format='GIF', append_images=image_frames[1:], \n",
    "                     save_all=True,\n",
    "                     duration=1000, \n",
    "                     loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numclusters</th>\n",
       "      <th>edges_correct</th>\n",
       "      <th>edges_missing</th>\n",
       "      <th>edges_to_remove</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.742857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  numclusters edges_correct edges_missing edges_to_remove        F1\n",
       "0           9             8            12               0  0.571429\n",
       "1           8             9            11               0  0.620690\n",
       "2           7            10            10               0  0.666667\n",
       "3           6            11             9               0  0.709677\n",
       "4           5            12             8               0  0.750000\n",
       "5           4            13             7               0  0.787879\n",
       "6           3            13             7               1  0.764706\n",
       "7           2            13             7               2  0.742857\n",
       "8           1            13             7               3  0.722222"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['numclusters', 'edges_correct','edges_missing', 'edges_to_remove', 'F1']][:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {frozenset({'atemp',\n",
       "                        'casual',\n",
       "                        'cnt',\n",
       "                        'demand in day -1',\n",
       "                        'demand in day -10',\n",
       "                        'demand in day -11',\n",
       "                        'demand in day -12',\n",
       "                        'demand in day -2',\n",
       "                        'demand in day -3',\n",
       "                        'demand in day -4',\n",
       "                        'demand in day -5',\n",
       "                        'demand in day -6',\n",
       "                        'demand in day -7',\n",
       "                        'demand in day -8',\n",
       "                        'demand in day -9',\n",
       "                        'demand in hour -1',\n",
       "                        'demand in hour -10',\n",
       "                        'demand in hour -11',\n",
       "                        'demand in hour -12',\n",
       "                        'demand in hour -2',\n",
       "                        'demand in hour -3',\n",
       "                        'demand in hour -4',\n",
       "                        'demand in hour -5',\n",
       "                        'demand in hour -6',\n",
       "                        'demand in hour -7',\n",
       "                        'demand in hour -8',\n",
       "                        'demand in hour -9',\n",
       "                        'demand in week -1',\n",
       "                        'demand in week -10',\n",
       "                        'demand in week -11',\n",
       "                        'demand in week -12',\n",
       "                        'demand in week -2',\n",
       "                        'demand in week -3',\n",
       "                        'demand in week -4',\n",
       "                        'demand in week -5',\n",
       "                        'demand in week -6',\n",
       "                        'demand in week -7',\n",
       "                        'demand in week -8',\n",
       "                        'demand in week -9',\n",
       "                        'dteday',\n",
       "                        'holiday',\n",
       "                        'hr',\n",
       "                        'hum',\n",
       "                        'mnth',\n",
       "                        'registered',\n",
       "                        'season',\n",
       "                        'temp',\n",
       "                        'weathersit',\n",
       "                        'weekday',\n",
       "                        'windspeed',\n",
       "                        'workingday',\n",
       "                        'yr'}): ['v1.csv',\n",
       "              'v3.csv',\n",
       "              'v2.csv',\n",
       "              'v4.csv',\n",
       "              'v5.csv',\n",
       "              'v6.csv',\n",
       "              'v11-s.csv',\n",
       "              'v10.csv',\n",
       "              'v7.csv',\n",
       "              'v8.csv',\n",
       "              'v9.csv',\n",
       "              'v16.csv',\n",
       "              'v18.csv',\n",
       "              'v17.csv',\n",
       "              'v20.csv',\n",
       "              'v19.csv',\n",
       "              'v12.csv',\n",
       "              'v13.csv',\n",
       "              'v11-R.csv',\n",
       "              'v14.csv',\n",
       "              'v15.csv']})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df_long.csv':    int-numbers str-numbers lc-letter uc-letter\n",
       " 0            1         one         a         A\n",
       " 1            2         two         b         B\n",
       " 2            3       three         c         C\n",
       " 3            4        four         d         D\n",
       " 4            5        five         e         E,\n",
       " 'ab_students.csv':      names letter_grades  number_grades\n",
       " 0     Alma             A            100\n",
       " 1  Anthony             A             95\n",
       " 2      Ava             A             93\n",
       " 3    Barry             B             88\n",
       " 4    Brick             B             87\n",
       " 5    Betty             B             89,\n",
       " 'cd_students.csv':     names letter_grades  number_grades\n",
       " 0     Cam             C             79\n",
       " 1  Caroly             C             79\n",
       " 2   Cathy             C             76\n",
       " 3   David             D             69\n",
       " 4  Darius             D             66\n",
       " 5   Dipsy             D             68,\n",
       " 'sdf.csv':    letter  length\n",
       " 0       A       7\n",
       " 1       A       6\n",
       " 2       A       7\n",
       " 3       A       8\n",
       " 4       C      10\n",
       " 5       C       8\n",
       " 6       C      11\n",
       " 7       D       8\n",
       " 8       F       7\n",
       " 9       G       7\n",
       " 10      H       6\n",
       " 11      I       5\n",
       " 12      I       8\n",
       " 13      I       7\n",
       " 14      I       4\n",
       " 15      K       6\n",
       " 16      K       8\n",
       " 17      L       9\n",
       " 18      M       5\n",
       " 19      M       8\n",
       " 20      M      13\n",
       " 21      M       8\n",
       " 22      M       9\n",
       " 23      M      11\n",
       " 24      M       8\n",
       " 25      M       7\n",
       " 0       N       8\n",
       " 1       N       6\n",
       " 2       N      13\n",
       " 3       N      10\n",
       " 4       N      10\n",
       " 5       N       8\n",
       " 6       N      14\n",
       " 7       N      12\n",
       " 8       O       4\n",
       " 9       O       8\n",
       " 10      O       6\n",
       " 11      P      12\n",
       " 12      R      12\n",
       " 13      S      14\n",
       " 14      S      12\n",
       " 15      T       9\n",
       " 16      T       5\n",
       " 17      U       4\n",
       " 18      V       7\n",
       " 19      V       8\n",
       " 20      W      10\n",
       " 21      W      13\n",
       " 22      W       9\n",
       " 23      W       7,\n",
       " 'states.csv':    abbreviation            name\n",
       " 0            AL         Alabama\n",
       " 1            AK          Alaska\n",
       " 2            AZ         Arizona\n",
       " 3            AR        Arkansas\n",
       " 4            CA      California\n",
       " 5            CO        Colorado\n",
       " 6            CT     Connecticut\n",
       " 7            DE        Delaware\n",
       " 8            FL         Florida\n",
       " 9            GA         Georgia\n",
       " 10           HI          Hawaii\n",
       " 11           ID           Idaho\n",
       " 12           IL        Illinois\n",
       " 13           IN         Indiana\n",
       " 14           IA            Iowa\n",
       " 15           KS          Kansas\n",
       " 16           KY        Kentucky\n",
       " 17           LA       Louisiana\n",
       " 18           ME           Maine\n",
       " 19           MD        Maryland\n",
       " 20           MA   Massachusetts\n",
       " 21           MI        Michigan\n",
       " 22           MN       Minnesota\n",
       " 23           MS     Mississippi\n",
       " 24           MO        Missouri\n",
       " 25           MT         Montana\n",
       " 0            NE        Nebraska\n",
       " 1            NV          Nevada\n",
       " 2            NH   New Hampshire\n",
       " 3            NJ      New Jersey\n",
       " 4            NM      New Mexico\n",
       " 5            NY        New York\n",
       " 6            NC  North Carolina\n",
       " 7            ND    North Dakota\n",
       " 8            OH            Ohio\n",
       " 9            OK        Oklahoma\n",
       " 10           OR          Oregon\n",
       " 11           PA    Pennsylvania\n",
       " 12           RI    Rhode Island\n",
       " 13           SC  South Carolina\n",
       " 14           SD    South Dakota\n",
       " 15           TN       Tennessee\n",
       " 16           TX           Texas\n",
       " 17           UT            Utah\n",
       " 18           VT         Vermont\n",
       " 19           VA        Virginia\n",
       " 20           WA      Washington\n",
       " 21           WV   West Virginia\n",
       " 22           WI       Wisconsin\n",
       " 23           WY         Wyoming,\n",
       " 'capitals.csv':            capital   latitude   longitude            name\n",
       " 0       Montgomery  32.377716  -86.300568         Alabama\n",
       " 1           Juneau  58.301598 -134.420212          Alaska\n",
       " 2          Phoenix  33.448143 -112.096962         Arizona\n",
       " 3      Little Rock  34.746613  -92.288986        Arkansas\n",
       " 4       Sacramento  38.576668 -121.493629      California\n",
       " 5           Denver  39.739227 -104.984856        Colorado\n",
       " 6         Hartford  41.764046  -72.682198     Connecticut\n",
       " 7            Dover  39.157307  -75.519722        Delaware\n",
       " 8         Honolulu  21.307442 -157.857376          Hawaii\n",
       " 9      Tallahassee  30.438118  -84.281296         Florida\n",
       " 10         Atlanta  33.749027  -84.388229         Georgia\n",
       " 11           Boise  43.617775 -116.199722           Idaho\n",
       " 12     Springfield  39.798363  -89.654961        Illinois\n",
       " 13    Indianapolis  39.768623  -86.162643         Indiana\n",
       " 14      Des Moines  41.591087  -93.603729            Iowa\n",
       " 15          Topeka  39.048191  -95.677956          Kansas\n",
       " 16       Frankfort  38.186722  -84.875374        Kentucky\n",
       " 17     Baton Rouge  30.457069  -91.187393       Louisiana\n",
       " 18         Augusta  44.307167  -69.781693           Maine\n",
       " 19       Annapolis  38.978764  -76.490936        Maryland\n",
       " 20          Boston  42.358162  -71.063698   Massachusetts\n",
       " 21         Lansing  42.733635  -84.555328        Michigan\n",
       " 22        St. Paul  44.955097  -93.102211       Minnesota\n",
       " 23         Jackson  32.303848  -90.182106     Mississippi\n",
       " 24  Jefferson City  38.579201  -92.172935        Missouri\n",
       " 25          Helena  46.585709 -112.018417         Montana\n",
       " 26         Lincoln  40.808075  -96.699654        Nebraska\n",
       " 27     Carson City  39.163914 -119.766121          Nevada\n",
       " 28         Concord  43.206898  -71.537994   New Hampshire\n",
       " 29         Trenton  40.220596  -74.769913      New Jersey\n",
       " 30        Santa Fe  35.682240 -105.939728      New Mexico\n",
       " 31         Raleigh  35.780430  -78.639099  North Carolina\n",
       " 32        Bismarck  46.820850 -100.783318    North Dakota\n",
       " 33          Albany  42.652843  -73.757874        New York\n",
       " 34        Columbus  39.961346  -82.999069            Ohio\n",
       " 35   Oklahoma City  35.492207  -97.503342        Oklahoma\n",
       " 36           Salem  44.938461 -123.030403          Oregon\n",
       " 37      Harrisburg  40.264378  -76.883598    Pennsylvania\n",
       " 38      Providence  41.830914  -71.414963    Rhode Island\n",
       " 39        Columbia  34.000343  -81.033211  South Carolina\n",
       " 40          Pierre  44.367031 -100.346405    South Dakota\n",
       " 41       Nashville  36.165810  -86.784241       Tennessee\n",
       " 42          Austin  30.274670  -97.740349           Texas\n",
       " 43  Salt Lake City  40.777477 -111.888237            Utah\n",
       " 44      Montpelier  44.262436  -72.580536         Vermont\n",
       " 45        Richmond  37.538857  -77.433640        Virginia\n",
       " 46         Olympia  47.035805 -122.905014      Washington\n",
       " 47      Charleston  38.336246  -81.612328   West Virginia\n",
       " 48         Madison  43.074684  -89.384445       Wisconsin\n",
       " 49        Cheyenne  41.140259 -104.820236         Wyoming,\n",
       " 'states_atom.csv':    abbreviation           name\n",
       " 0            AL        Alabama\n",
       " 1            AK         Alaska\n",
       " 2            AZ        Arizona\n",
       " 3            AR       Arkansas\n",
       " 4            CA     California\n",
       " 5            CO       Colorado\n",
       " 6            CT    Connecticut\n",
       " 7            DE       Delaware\n",
       " 8            FL        Florida\n",
       " 9            GA        Georgia\n",
       " 10           HI         Hawaii\n",
       " 11           ID          Idaho\n",
       " 12           IL       Illinois\n",
       " 13           IN        Indiana\n",
       " 14           IA           Iowa\n",
       " 15           KS         Kansas\n",
       " 16           KY       Kentucky\n",
       " 17           LA      Louisiana\n",
       " 18           ME          Maine\n",
       " 19           MD       Maryland\n",
       " 20           MA  Massachusetts\n",
       " 21           MI       Michigan\n",
       " 22           MN      Minnesota\n",
       " 23           MS    Mississippi\n",
       " 24           MO       Missouri\n",
       " 25           MT        Montana,\n",
       " 'states_ntoz.csv':    abbreviation            name\n",
       " 0            NE        Nebraska\n",
       " 1            NV          Nevada\n",
       " 2            NH   New Hampshire\n",
       " 3            NJ      New Jersey\n",
       " 4            NM      New Mexico\n",
       " 5            NY        New York\n",
       " 6            NC  North Carolina\n",
       " 7            ND    North Dakota\n",
       " 8            OH            Ohio\n",
       " 9            OK        Oklahoma\n",
       " 10           OR          Oregon\n",
       " 11           PA    Pennsylvania\n",
       " 12           RI    Rhode Island\n",
       " 13           SC  South Carolina\n",
       " 14           SD    South Dakota\n",
       " 15           TN       Tennessee\n",
       " 16           TX           Texas\n",
       " 17           UT            Utah\n",
       " 18           VT         Vermont\n",
       " 19           VA        Virginia\n",
       " 20           WA      Washington\n",
       " 21           WV   West Virginia\n",
       " 22           WI       Wisconsin\n",
       " 23           WY         Wyoming,\n",
       " 'bdf.csv':    abbreviation            name first_letter last_letter  name_length\n",
       " 0            AL         Alabama            A           A            7\n",
       " 1            AK          Alaska            A           A            6\n",
       " 2            AZ         Arizona            A           A            7\n",
       " 3            AR        Arkansas            A           S            8\n",
       " 4            CA      California            C           A           10\n",
       " 5            CO        Colorado            C           O            8\n",
       " 6            CT     Connecticut            C           T           11\n",
       " 7            DE        Delaware            D           E            8\n",
       " 8            FL         Florida            F           A            7\n",
       " 9            GA         Georgia            G           A            7\n",
       " 10           HI          Hawaii            H           I            6\n",
       " 11           ID           Idaho            I           O            5\n",
       " 12           IL        Illinois            I           S            8\n",
       " 13           IN         Indiana            I           A            7\n",
       " 14           IA            Iowa            I           A            4\n",
       " 15           KS          Kansas            K           S            6\n",
       " 16           KY        Kentucky            K           Y            8\n",
       " 17           LA       Louisiana            L           A            9\n",
       " 18           ME           Maine            M           E            5\n",
       " 19           MD        Maryland            M           D            8\n",
       " 20           MA   Massachusetts            M           S           13\n",
       " 21           MI        Michigan            M           N            8\n",
       " 22           MN       Minnesota            M           A            9\n",
       " 23           MS     Mississippi            M           I           11\n",
       " 24           MO        Missouri            M           I            8\n",
       " 25           MT         Montana            M           A            7\n",
       " 0            NE        Nebraska            N           A            8\n",
       " 1            NV          Nevada            N           A            6\n",
       " 2            NH   New Hampshire            N           E           13\n",
       " 3            NJ      New Jersey            N           Y           10\n",
       " 4            NM      New Mexico            N           O           10\n",
       " 5            NY        New York            N           K            8\n",
       " 6            NC  North Carolina            N           A           14\n",
       " 7            ND    North Dakota            N           A           12\n",
       " 8            OH            Ohio            O           O            4\n",
       " 9            OK        Oklahoma            O           A            8\n",
       " 10           OR          Oregon            O           N            6\n",
       " 11           PA    Pennsylvania            P           A           12\n",
       " 12           RI    Rhode Island            R           D           12\n",
       " 13           SC  South Carolina            S           A           14\n",
       " 14           SD    South Dakota            S           A           12\n",
       " 15           TN       Tennessee            T           E            9\n",
       " 16           TX           Texas            T           S            5\n",
       " 17           UT            Utah            U           H            4\n",
       " 18           VT         Vermont            V           T            7\n",
       " 19           VA        Virginia            V           A            8\n",
       " 20           WA      Washington            W           N           10\n",
       " 21           WV   West Virginia            W           A           13\n",
       " 22           WI       Wisconsin            W           N            9\n",
       " 23           WY         Wyoming            W           G            7,\n",
       " 'df_short.csv':    int-numbers phonetics letters\n",
       " 0            1     alpha       a\n",
       " 1            2     bravo       b\n",
       " 2            3   charlie       c\n",
       " 3           26      zulu       z}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "wf_dir = BASE_DIR+NB_NAME\n",
    "g_truth = nx.read_gpickle(wf_dir+'/'+NB_NAME+'_gt_fixed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdf.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in nx.topological_sort(g_truth)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
