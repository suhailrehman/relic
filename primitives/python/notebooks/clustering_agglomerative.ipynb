{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow:\n",
    "\n",
    "1. We first do exact schema clustering\n",
    "2. Apply PPO within each cluster\n",
    "3. Merge clusters one at a time to connect\n",
    "    1. Use PPO First\n",
    "    2. If schema match during merge, try join check\n",
    "    3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lineage import graphs, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_DIR = '/media/suhail/Data/experiments/reexec/res/'\n",
    "BASE_DIR = '/home/suhail/Projects/sample_workflows/million_notebooks/selected/'\n",
    "#BASE_DIR = '/home/suhail/Projects/relic/primitives/python/generator/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_NAME = 'nb_331056.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "# Jaccard Distance First\n",
    "\n",
    "#Duplicate function\n",
    "def set_jaccard_distance(set1,set2):\n",
    "    intersect = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return 1-(len(intersect)/len(union))\n",
    "\n",
    "def select_merge_candidates(schemas):\n",
    "    distance_dict = defaultdict(list)\n",
    "    for combo in itertools.combinations(schemas,2):\n",
    "        distance_dict[set_jaccard_distance(*combo)].append(combo)\n",
    "    \n",
    "    return distance_dict[min(distance_dict)], min(distance_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(schema_dict, schema1, schema2):\n",
    "    \n",
    "    union = schema1.union(schema2)\n",
    "    elements = schema_dict[schema1] + schema_dict[schema2] \n",
    "    \n",
    "    #print('BEFORE')\n",
    "    #print(schema_dict)\n",
    "    #print(union)\n",
    "    #print(schema_dict[union])\n",
    "    \n",
    "    try:\n",
    "        del(schema_dict[schema1])\n",
    "        del(schema_dict[schema2])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    schema_dict[union] = elements\n",
    "    #print('AFTER')\n",
    "    #print(schema_dict)\n",
    "    \n",
    "    return schema_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as ds\n",
    "import clustering\n",
    "\n",
    "\n",
    "df_dict = ds.build_df_dict_dir(BASE_DIR+NB_NAME+'/artifacts/')\n",
    "df_dict\n",
    "schema_dict = clustering.exact_schema_cluster(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates,score = select_merge_candidates(schema_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level\n",
      "level\n",
      "level\n"
     ]
    }
   ],
   "source": [
    "while(len(schema_dict) > 1):\n",
    "    print('level')\n",
    "    schemas, score = select_merge_candidates(schema_dict.keys())\n",
    "    schema_dict = merge_clusters(schema_dict, schemas[0][0], schemas[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {frozenset({'amount',\n",
       "                        'calaccess_committee_id',\n",
       "                        'calaccess_filing_id',\n",
       "                        'calaccess_prop_id',\n",
       "                        'ccdc_committee_id',\n",
       "                        'ccdc_prop_id',\n",
       "                        'committee_name',\n",
       "                        'committee_name_x',\n",
       "                        'committee_name_y',\n",
       "                        'committee_position',\n",
       "                        'contributor_city',\n",
       "                        'contributor_employer',\n",
       "                        'contributor_firstname',\n",
       "                        'contributor_fullname',\n",
       "                        'contributor_is_self_employed',\n",
       "                        'contributor_lastname',\n",
       "                        'contributor_occupation',\n",
       "                        'contributor_state',\n",
       "                        'contributor_zip',\n",
       "                        'date_received',\n",
       "                        'ocd_prop_id',\n",
       "                        'prop_name'}): ['top_supporters.csv',\n",
       "              'props.csv',\n",
       "              'prop.csv',\n",
       "              'oppose.csv',\n",
       "              'noncacontribs.csv',\n",
       "              'support.csv',\n",
       "              'merged.csv',\n",
       "              'contribs.csv']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lineage import similarity, graphs\n",
    "\n",
    "def intra_cluster_similarity(df_dict, clusters, threshold=0.0001):\n",
    "    pairwise_jaccard = []\n",
    "    for cluster in clusters.values():\n",
    "        batch = {k: df_dict[k] for k in cluster}\n",
    "        pw_batch = similarity.get_pairwise_similarity(batch, similarity.compute_jaccard_DF, threshold=threshold)\n",
    "        pairwise_jaccard.extend(pw_batch)\n",
    "    return pairwise_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import nppo\n",
    "\n",
    "\n",
    "def lineage_inference_agglomerative(nb_name=NB_NAME, base_dir=BASE_DIR,\n",
    "                      pre_cluster=False,\n",
    "                      index=True, threshold=0.0001,\n",
    "                      join_edges=False,\n",
    "                      group_edges=False,\n",
    "                      ):\n",
    "\n",
    "    wf_dir = base_dir+nb_name\n",
    "\n",
    "    if index:\n",
    "        artifact_dir = wf_dir+'/artifacts/'\n",
    "    else:\n",
    "        artifact_dir = wf_dir+'/artifacts_1/'\n",
    "\n",
    "    #Output Directory\n",
    "    result_dir = wf_dir+'/inferred/'\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # Output Files\n",
    "    schema_file = result_dir+'schema_matching.csv'\n",
    "    row_file = result_dir+'row_matching.csv'\n",
    "    cluster_file = result_dir+'clusters.csv'\n",
    "    \n",
    "    \n",
    "    # Prepare Dataframe for results\n",
    "    pr_df = pd.DataFrame(columns = ['nb_name', 'index', 'numclusters',  \n",
    "                                    'distance_metric', 'edges_correct', \n",
    "                                    'edges_missing', 'edges_to_remove',\n",
    "                                    'join_edges', 'precision', 'recall', 'F1',\n",
    "                                    'missing_files'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Load Dataset\n",
    "    dataset = ds.build_df_dict_dir(artifact_dir)\n",
    "    \n",
    "    \n",
    "    # Load Ground Truth:\n",
    "    g_truth = nx.read_gpickle(wf_dir+'/'+nb_name+'_gt_fixed.pkl')\n",
    "    \n",
    "    # Check for files in the ground truth that are missing in file list\n",
    "    missing_files = ds.check_csv_graph(artifact_dir, g_truth)\n",
    "\n",
    "    # Cluster for visualization\n",
    "    clusters = clustering.exact_schema_cluster(dataset)\n",
    "    clustering.write_clusters_to_file(clusters, result_dir+'clusters_with_filename.csv')\n",
    "\n",
    "    # Start with intra-cluster edges:\n",
    "    pairwise_jaccard = intra_cluster_similarity(df_dict, clusters)\n",
    "\n",
    "    pw_jaccard_graph = graphs.generate_pairwise_graph(pairwise_jaccard)\n",
    "\n",
    "    \n",
    "       \n",
    "    # Write out the Pairwise Distances as Adj list\n",
    "    nx.to_pandas_adjacency(pw_jaccard_graph,weight='weight').to_csv(\n",
    "                                                result_dir+'cell_sim.csv')\n",
    "\n",
    "    g_inferred = graphs.generate_spanning_tree(pw_jaccard_graph)\n",
    "    nx.write_edgelist(g_inferred,result_dir+'infered_mst_cell.csv',data=True)\n",
    "    \n",
    "    #Draw first graph and get results\n",
    "    cluster_dict = clustering.get_graph_clusters(result_dir+'clusters_with_filename.csv')\n",
    "    img_frames.append(graphs.generate_and_draw_graph(base_dir, nb_name, 'cell', cluster_dict=cluster_dict, join_list=None))\n",
    "    \n",
    "\n",
    "    result = graphs.get_precision_recall(g_truth,g_inferred)\n",
    "        \n",
    "    pr_df = pr_df.append({\n",
    "        'nb_name': nb_name,\n",
    "        'index': index,\n",
    "        'numclusters': len(clusters),\n",
    "        'distance_metric': 'pandas_cell',\n",
    "        'edges_correct': len(result['correct_edges']),\n",
    "        'edges_missing': len(result['to_add']),\n",
    "        'edges_to_remove': len(result['to_remove']),\n",
    "        #'join_edges': len(inferred_j_edges),\n",
    "        'precision': result['Precision'],\n",
    "        'recall': result['Recall'],\n",
    "        'F1': result['F1'],\n",
    "        'missing_files': len(missing_files)\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    # Write out inferred graph\n",
    "    nx.write_edgelist(g_inferred,result_dir+'infered_mst_cell.csv',data=True)\n",
    "\n",
    "    #Clustering Loop Starts here\n",
    "    while(len(clusters) > 1):\n",
    "        print('Num Clusters:', len(clusters))\n",
    "        candidates,score = select_merge_candidates(clusters.keys())\n",
    "\n",
    "        clusterset1, clusterset2 = clusters[candidates[0][0]], clusters[candidates[0][1]]\n",
    "\n",
    "        src, dst, score = similarity.get_pairs_similarity(df_dict, clusterset1, clusterset2)[0]\n",
    "        print('Adding Edge:', src, dst, score)\n",
    "        \n",
    "        if(score > 0):\n",
    "            g_inferred.add_edge(src, dst, weight=score)\n",
    "\n",
    "        nx.write_edgelist(g_inferred,result_dir+'infered_mst_cell.csv',data=True)\n",
    "        clusters = merge_clusters(clusters, candidates[0][0], candidates[0][1])\n",
    "        \n",
    "        # Draw inferred graph image:\n",
    "        if(len(clusters) > 1):\n",
    "            clustering.write_clusters_to_file(clusters, result_dir+'clusters_with_filename.csv')\n",
    "            cluster_dict = clustering.get_graph_clusters(result_dir+'clusters_with_filename.csv')\n",
    "        else:\n",
    "            cluster_dict = None\n",
    "        img_frames.append(graphs.generate_and_draw_graph(base_dir, nb_name, 'cell', cluster_dict=cluster_dict, join_list=None))\n",
    "    \n",
    "    \n",
    "        #Compute PR after merge\n",
    "        \n",
    "        result = graphs.get_precision_recall(g_truth,g_inferred)\n",
    "        \n",
    "        pr_df = pr_df.append({\n",
    "            'nb_name': nb_name,\n",
    "            'index': index,\n",
    "            'numclusters': len(clusters),\n",
    "            'distance_metric': 'pandas_cell',\n",
    "            'edges_correct': len(result['correct_edges']),\n",
    "            'edges_missing': len(result['to_add']),\n",
    "            'edges_to_remove': len(result['to_remove']),\n",
    "            #'join_edges': len(inferred_j_edges),\n",
    "            'precision': result['Precision'],\n",
    "            'recall': result['Recall'],\n",
    "            'F1': result['F1'],\n",
    "            'missing_files': len(missing_files)\n",
    "        }, ignore_index=True)\n",
    "            \n",
    "\n",
    "    # Test for NPPOs:\n",
    "    \n",
    "    inferred_j_edges = []\n",
    "    join_list = None\n",
    "    cluster_dict = None\n",
    "\n",
    "    if join_edges:\n",
    "        print('Writing Cluster File')\n",
    "\n",
    "        print(\"Adding Join Edges\")\n",
    "        join_list = nppo.find_all_joins_df_dict(dataset)\n",
    "        print(len(join_list), \"Joins Detected\")\n",
    "        g_inferred = nppo.add_join_edges(join_list, g_inferred)\n",
    "\n",
    "\n",
    "        for join in join_list:\n",
    "            inferred_j_edges.append((join[0], join[2]))\n",
    "            inferred_j_edges.append((join[1], join[2]))\n",
    "\n",
    "\n",
    "        nppo.write_join_candidates(join_list, result_dir+'join_candidates.csv')\n",
    "\n",
    "        g_truth_j_edges = [(u,v) for u,v,d in g_truth.edges(data=True) \\\n",
    "                           if g_truth[u][v]['operation'] == 'merge' ]\n",
    "\n",
    "        #Check Join Precision/Recall\n",
    "        #print(get_join_precision_recall(g_truth_j_edges, inferred_j_edges))\n",
    "        \n",
    "        result = graphs.get_precision_recall(g_truth,g_inferred)\n",
    "\n",
    "        cluster_dict = clustering.get_graph_clusters(result_dir+'clusters_with_filename.csv')\n",
    "        img_frames.append(graphs.generate_and_draw_graph(base_dir, nb_name, 'cell', cluster_dict=cluster_dict, join_list=join_list))\n",
    "\n",
    "        pr_df = pr_df.append({\n",
    "            'nb_name': nb_name,\n",
    "            'index': index,\n",
    "            'numclusters': len(clusters),\n",
    "            'distance_metric': 'pandas_cell',\n",
    "            'edges_correct': len(result['correct_edges']),\n",
    "            'edges_missing': len(result['to_add']),\n",
    "            'edges_to_remove': len(result['to_remove']),\n",
    "            'join_edges': len(inferred_j_edges),\n",
    "            'precision': result['Precision'],\n",
    "            'recall': result['Recall'],\n",
    "            'F1': result['F1'],\n",
    "            'missing_files': len(missing_files)\n",
    "        }, ignore_index=True)\n",
    "  \n",
    "    return pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='graph pairs', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='graph pairs', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='graph pairs', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='graph pairs', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Clusters: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='cluster pairs', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Edge: merged.csv contribs.csv 0.0015397492649661011\n",
      "Num Clusters: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='cluster pairs', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Edge: props.csv support.csv 0.0073740782402199724\n",
      "Num Clusters: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='cluster pairs', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Adding Edge: top_supporters.csv props.csv 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_name</th>\n",
       "      <th>index</th>\n",
       "      <th>numclusters</th>\n",
       "      <th>distance_metric</th>\n",
       "      <th>edges_correct</th>\n",
       "      <th>edges_missing</th>\n",
       "      <th>edges_to_remove</th>\n",
       "      <th>join_edges</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>missing_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nb_495072.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>pandas_cell</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nb_name index numclusters distance_metric edges_correct  \\\n",
       "0  nb_495072.ipynb  True           4     pandas_cell             2   \n",
       "1  nb_495072.ipynb  True           3     pandas_cell             3   \n",
       "2  nb_495072.ipynb  True           2     pandas_cell             3   \n",
       "3  nb_495072.ipynb  True           1     pandas_cell             3   \n",
       "\n",
       "  edges_missing edges_to_remove  join_edges  precision  recall        F1  \\\n",
       "0             5               2         NaN   0.285714     0.5  0.363636   \n",
       "1             4               2         NaN   0.428571     0.6  0.500000   \n",
       "2             4               3         NaN   0.428571     0.5  0.461538   \n",
       "3             4               3         NaN   0.428571     0.5  0.461538   \n",
       "\n",
       "  missing_files  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x3600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = lineage_inference_agglomerative(nb_name=NB_NAME)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_frames = [Image.open(frame) for frame in img_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_frames[0].save('mexican.gif', \n",
    "                     format='GIF', append_images=image_frames[1:], \n",
    "                     save_all=True,\n",
    "                     duration=1000, \n",
    "                     loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numclusters</th>\n",
       "      <th>edges_correct</th>\n",
       "      <th>edges_missing</th>\n",
       "      <th>edges_to_remove</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.742857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  numclusters edges_correct edges_missing edges_to_remove        F1\n",
       "0           9             8            12               0  0.571429\n",
       "1           8             9            11               0  0.620690\n",
       "2           7            10            10               0  0.666667\n",
       "3           6            11             9               0  0.709677\n",
       "4           5            12             8               0  0.750000\n",
       "5           4            13             7               0  0.787879\n",
       "6           3            13             7               1  0.764706\n",
       "7           2            13             7               2  0.742857\n",
       "8           1            13             7               3  0.722222"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['numclusters', 'edges_correct','edges_missing', 'edges_to_remove', 'F1']][:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {frozenset({'atemp',\n",
       "                        'casual',\n",
       "                        'cnt',\n",
       "                        'demand in day -1',\n",
       "                        'demand in day -10',\n",
       "                        'demand in day -11',\n",
       "                        'demand in day -12',\n",
       "                        'demand in day -2',\n",
       "                        'demand in day -3',\n",
       "                        'demand in day -4',\n",
       "                        'demand in day -5',\n",
       "                        'demand in day -6',\n",
       "                        'demand in day -7',\n",
       "                        'demand in day -8',\n",
       "                        'demand in day -9',\n",
       "                        'demand in hour -1',\n",
       "                        'demand in hour -10',\n",
       "                        'demand in hour -11',\n",
       "                        'demand in hour -12',\n",
       "                        'demand in hour -2',\n",
       "                        'demand in hour -3',\n",
       "                        'demand in hour -4',\n",
       "                        'demand in hour -5',\n",
       "                        'demand in hour -6',\n",
       "                        'demand in hour -7',\n",
       "                        'demand in hour -8',\n",
       "                        'demand in hour -9',\n",
       "                        'demand in week -1',\n",
       "                        'demand in week -10',\n",
       "                        'demand in week -11',\n",
       "                        'demand in week -12',\n",
       "                        'demand in week -2',\n",
       "                        'demand in week -3',\n",
       "                        'demand in week -4',\n",
       "                        'demand in week -5',\n",
       "                        'demand in week -6',\n",
       "                        'demand in week -7',\n",
       "                        'demand in week -8',\n",
       "                        'demand in week -9',\n",
       "                        'dteday',\n",
       "                        'holiday',\n",
       "                        'hr',\n",
       "                        'hum',\n",
       "                        'mnth',\n",
       "                        'registered',\n",
       "                        'season',\n",
       "                        'temp',\n",
       "                        'weathersit',\n",
       "                        'weekday',\n",
       "                        'windspeed',\n",
       "                        'workingday',\n",
       "                        'yr'}): ['v1.csv',\n",
       "              'v3.csv',\n",
       "              'v2.csv',\n",
       "              'v4.csv',\n",
       "              'v5.csv',\n",
       "              'v6.csv',\n",
       "              'v11-s.csv',\n",
       "              'v10.csv',\n",
       "              'v7.csv',\n",
       "              'v8.csv',\n",
       "              'v9.csv',\n",
       "              'v16.csv',\n",
       "              'v18.csv',\n",
       "              'v17.csv',\n",
       "              'v20.csv',\n",
       "              'v19.csv',\n",
       "              'v12.csv',\n",
       "              'v13.csv',\n",
       "              'v11-R.csv',\n",
       "              'v14.csv',\n",
       "              'v15.csv']})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
